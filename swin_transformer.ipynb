{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44f65f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1、如何基于图片生成patch embedding?\n",
    "### 方法一\n",
    "- 基于pytorch unfold的API来将图片进行分块，也就是模仿卷积的思路，设置kernel_size=stride=patch_size，得到分块后的图片\n",
    "- 得到格式为[bs, num_patch, patch_depth]的张量\n",
    "- 将张量与形状为[patch_depth, model_dim_C]的权重矩阵进行乘法操作，即可得到形状为[bs, num_patch, model_dim_C]的patch embedding\n",
    "\n",
    "### 方法二\n",
    "- patch_depth是等于input_channel\\*patch_size\\*patch_size\n",
    "- model_dim_C相当于二维卷积的输出通道数目\n",
    "- 将形状为[patch_depth, model_dim_C]的权重矩阵转换为[model_dim_C, input_channel, patch_size, patch_size]的卷积核\n",
    "- 调用PyTorch的conv2d API得到卷积的输出张量，形状为[bs, output_channel, height, width]\n",
    "- 转换为[bs, num_patch, model_dim_C]的格式，即为patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a51bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 难点1 patch embedding\n",
    "def image2emb_naive(image, patch_size, weight):\n",
    "    \"\"\" 直观方法去实现patch embedding \"\"\"\n",
    "    # image shape: bs*channel*h*w\n",
    "    patch = F.unfold(image, kernel_size=(patch_size, patch_size),\n",
    "                     stride=(patch_size, patch_size)).transpose(-1, -2) #[bs, num_patch, patch_depth]\n",
    "    patch_embedding = patch @ weight #[bs,num_patch,model_dim_C]\n",
    "    return patch_embedding\n",
    "\n",
    "def image2emb_conv(image, kernel, stride):\n",
    "    \"\"\" 基于二维卷积来实现patch embedding，embedding的维度就是卷积的输出通道数 \"\"\"\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride) # bs*oc*oh*ow\n",
    "    bs, oc, oh, ow = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh*ow)).transpose(-1, -2) #[bs,num_patch,model_dim_C]\n",
    "    return patch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488d7bd",
   "metadata": {},
   "source": [
    "## 2、如何构建MHSA并计算其复杂度？\n",
    "- 基于输入x进行三个映射分别得到q,k,v\n",
    "    - 此步复杂度为 $3LC^2$，其中L为序列长度，C为特征大小\n",
    "- 将q,k,v拆分成多头的形式，注意这里的多头各自计算不影响，所以可以与bs维度进行统一看待\n",
    "- 计算$q k^T$，并考虑可能的掩码，即让无效的两两位置之间的能量为负无穷，掩码是在shift window MHSA中会需要，而在window MHSA中暂不需要\n",
    "    - 此步复杂度为$L^2C$\n",
    "- 计算概率值与v的乘积\n",
    "    - 此步复杂度为$L^2C$\n",
    "- 对输出进行再次映射\n",
    "    - 此步复杂度为$LC^2$\n",
    "- 总体复杂度为$4LC^2+2L^2C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77481415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim, num_head):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        \n",
    "        self.proj_linear_layer = nn.Linear(model_dim, 3*model_dim)\n",
    "        self.final_linear_layer = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "    def forward(self, input, additive_mask=None):\n",
    "        bs, seqlen, model_dim = input.shape\n",
    "        num_head = self.num_head\n",
    "        head_dim = model_dim // num_head\n",
    "\n",
    "        proj_output = self.proj_linear_layer(input) # [bs, seqlen, 3*model_dim] \n",
    "        \n",
    "        q, k, v = proj_output.chunk(3, dim=-1) # 3*[bs, seqlen, model_dim] \n",
    "\n",
    "        q = q.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2) # [bs, num_head, seqlen, head_dim]\n",
    "        q = q.reshape(bs*num_head, seqlen, head_dim)\n",
    "\n",
    "        k = k.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2) # [bs, num_head, seqlen, head_dim]\n",
    "        k = k.reshape(bs*num_head, seqlen, head_dim)\n",
    "\n",
    "        v = v.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2) # [bs, num_head, seqlen, head_dim]\n",
    "        v = v.reshape(bs*num_head, seqlen, head_dim)\n",
    "\n",
    "        if additive_mask is None:\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/math.sqrt(head_dim), dim=-1)\n",
    "        else:\n",
    "            additive_mask = additive_mask.tile((num_head, 1, 1))\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/math.sqrt(head_dim)+additive_mask, dim=-1)\n",
    "\n",
    "        output = torch.bmm(attn_prob, v) # [bs*num_head, seqlen, head_dim]\n",
    "        output = output.reshape(bs, num_head, seqlen, head_dim).transpose(1, 2) #[bs, seqlen, num_head, head_dim]\n",
    "        output = output.reshape(bs, seqlen, model_dim)\n",
    "\n",
    "        output = self.final_linear_layer(output)\n",
    "        return attn_prob, output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c041b",
   "metadata": {},
   "source": [
    "## 3、如何构建Window MHSA并计算其复杂度？\n",
    "- 将patch组成的图片进一步划分成一个个更大的window\n",
    "    - 首先需要将三维的patch embedding转换成图片格式\n",
    "    - 使用unfold来将patch划分成window\n",
    "- 在每个window内部计算MHSA\n",
    "    - window数目其实可以跟batchsize进行统一对待，因为window与window之间没有交互计算\n",
    "    - 关于计算复杂度\n",
    "        - 假设窗的边长为W，那么计算每个窗的总体复杂度是$4W^2C^2+2W^4C$\n",
    "        - 假设patch的总数目为L，那么窗的数目为$L/W^2$\n",
    "        - 因此，W-MHSA的总体复杂度为$4LC^2+2LW^2C$\n",
    "    - 此处不需要mask\n",
    "    - 将计算结果转换成带window的四维张量格式\n",
    "- 复杂度对比\n",
    "    - **MHSA**: $4LC^2+2L^2C$\n",
    "    - **W-MHSA**：$4LC^2+2LW^2C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e1e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_multi_head_self_attention(patch_embedding, mhsa, window_size=4, num_head=2):\n",
    "    num_patch_in_window = window_size * window_size\n",
    "    bs, num_patch, patch_depth = patch_embedding.shape\n",
    "    image_height = image_width = int(math.sqrt(num_patch))\n",
    "    \n",
    "    patch_embedding = patch_embedding.transpose(-1, -2)\n",
    "    patch = patch_embedding.reshape(bs, patch_depth, image_height, image_width)\n",
    "    window = F.unfold(patch, kernel_size=(window_size, window_size),\n",
    "                      stride=(window_size, window_size)).transpose(-1, -2) #[bs, num_window, window_depth]\n",
    "    \n",
    "    bs, num_window, patch_depth_times_num_patch_in_window = window.shape\n",
    "    window = window.reshape(bs*num_window, patch_depth, num_patch_in_window).transpose(-1, -2) #[bs*num_w, num_patch, patch_depth]\n",
    "    \n",
    "    attn_prob, output = mhsa(window) #[bs*num_window, num_patch_in_window, patch_depth]\n",
    "    \n",
    "    output = output.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e70d5a9",
   "metadata": {},
   "source": [
    "## 4、如何构建Shift Window MHSA及其Mask？\n",
    "- 将上一步的W-MHSA的结果转换成图片格式\n",
    "- 假设已经做了新的window划分，这一步叫做shift-window\n",
    "- 为了保持window数目不变从而有高效的计算，需要将图片的patch往左和往上各自滑动半个窗口大小的步长，保持patch所属window类别不变\n",
    "- 将图片patch还原成window的数据格式\n",
    "- 由于cycle shift后，每个window虽然形状规整，但部分window中存在原本不属于同一个窗口的patch，所以需要生成mask\n",
    "- 如何生成mask？\n",
    "    - 首先构建一个shift-window的patch所属的window类别矩阵\n",
    "    - 对该矩阵进行同样的往左和往上各自滑动半个窗口大小的步长的操作\n",
    "    - 通过unfold操作得到[bs, num_window, num_patch_in_window]形状的类别矩阵\n",
    "    - 对该矩阵进行扩维成[bs, num_window, num_patch_in_window, 1]\n",
    "    - 将该矩阵与其转置矩阵进行作差，得到同类关系矩阵（为0的位置上的patch属于同类，否则属于不同类）\n",
    "    - 对同类关系矩阵中非零的位置用负无穷数进行填充，对于零的位置用0去填充，这样就构建好了MHSA所需要的mask\n",
    "    - 此mask的形状为[bs, num_window, num_patch_in_window, num_patch_in_window]\n",
    "- 将window转换成三维的格式，[bs\\*num_window, num_patch_in_window, patch_depth]\n",
    "- 将三维格式的特征连同mask一起送入MHSA中计算得到注意力输出\n",
    "- 将注意力输出转换成图片patch格式，[bs, num_window, num_patch_in_window, patch_depth]\n",
    "- 为了恢复位置，需要将图片的patch往右和往下各自滑动半个窗口大小的步长，至此，SW-MHSA计算完毕\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ce1f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个辅助函数, window2image，也就是将transformer block的结果转化成图片的格式\n",
    "def window2image(msa_output):\n",
    "    bs, num_window, num_patch_in_window, patch_depth = msa_output.shape\n",
    "    window_size = int(math.sqrt(num_patch_in_window))\n",
    "    image_height = int(math.sqrt(num_window)) * window_size\n",
    "    image_width = image_height\n",
    "    \n",
    "    msa_output = msa_output.reshape(bs, int(math.sqrt(num_window)),\n",
    "                                        int(math.sqrt(num_window)),\n",
    "                                        window_size,\n",
    "                                        window_size,\n",
    "                                        patch_depth)\n",
    "    msa_output = msa_output.transpose(2, 3)\n",
    "    image = msa_output.reshape(bs, image_height*image_width, patch_depth)\n",
    "    \n",
    "    image = image.transpose(-1, -2).reshape(bs, patch_depth, image_height, image_width) # 跟卷积格式一致\n",
    "    \n",
    "    return image\n",
    "\n",
    "# 定义辅助函数 shift_window，即高效地计算swmsa\n",
    "def shift_window(w_msa_output, window_size, shift_size, generate_mask=False):\n",
    "    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "    \n",
    "    w_msa_output = window2image(w_msa_output) #[bs, depth, h, w]\n",
    "    bs, patch_depth, image_height, image_width = w_msa_output.shape\n",
    "    \n",
    "    rolled_w_msa_output = torch.roll(w_msa_output, shifts=(shift_size, shift_size), dims=(2, 3))\n",
    "    \n",
    "    shifted_w_msa_input = rolled_w_msa_output.reshape(bs, patch_depth, \n",
    "                                                      int(math.sqrt(num_window)),\n",
    "                                                      window_size,\n",
    "                                                      int(math.sqrt(num_window)), \n",
    "                                                      window_size\n",
    "                                                     )\n",
    "    \n",
    "    shifted_w_msa_input = shifted_w_msa_input.transpose(3, 4)\n",
    "    shifted_w_msa_input = shifted_w_msa_input.reshape(bs, patch_depth, num_window*num_patch_in_window)\n",
    "    shifted_w_msa_input = shifted_w_msa_input.transpose(-1, -2) # [bs, num_window*num_patch_in_window, patch_depth]\n",
    "    shifted_window = shifted_w_msa_input.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    \n",
    "    if generate_mask:\n",
    "        additive_mask = build_mask_for_shifted_wmsa(bs, image_height, image_width, window_size)\n",
    "    else:\n",
    "        additive_mask = None\n",
    "    \n",
    "    return shifted_window, additive_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d404ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建shift window multi-head attention mask\n",
    "def build_mask_for_shifted_wmsa(batch_size, image_height, image_width, window_size):\n",
    "    index_matrix = torch.zeros(image_height, image_width)\n",
    "\n",
    "    for i in range(image_height):\n",
    "        for j in range(image_width):\n",
    "            row_times = (i+window_size//2) // window_size\n",
    "            col_times = (j+window_size//2) // window_size\n",
    "            index_matrix[i, j] = row_times*(image_height//window_size) + col_times + 1\n",
    "    rolled_index_matrix = torch.roll(index_matrix, shifts=(-window_size//2, -window_size//2), dims=(0, 1))\n",
    "    rolled_index_matrix = rolled_index_matrix.unsqueeze(0).unsqueeze(0) #[bs, ch, h, w]\n",
    "    \n",
    "    c = F.unfold(rolled_index_matrix, kernel_size=(window_size, window_size),\n",
    "                 stride=(window_size, window_size)).transpose(-1, -2) \n",
    "    \n",
    "    c = c.tile(batch_size, 1, 1) #[bs, num_window, num_patch_in_window]\n",
    "    \n",
    "    bs, num_window, num_patch_in_window = c.shape\n",
    "   \n",
    "    c1 = c.unsqueeze(-1) # [bs, num_window, num_patch_in_window, 1]\n",
    "    c2 = (c1 - c1.transpose(-1, -2)) == 0 #[bs, num_window, num_patch_in_window, num_patch_in_window]\n",
    "    valid_matrix = c2.to(torch.float32)    \n",
    "    additive_mask = (1-valid_matrix)*(-1e-9) #[bs, num_window, num_patch_in_window, num_patch_in_window]\n",
    "    \n",
    "    additive_mask = additive_mask.reshape(bs*num_window, num_patch_in_window, num_patch_in_window)\n",
    "    \n",
    "    return additive_mask\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e90194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shift_window_multi_head_self_attention(w_msa_output, mhsa, window_size=4, num_head=2):\n",
    "    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "    \n",
    "    shifted_w_msa_input, additive_mask = shift_window(w_msa_output, window_size,\n",
    "                                                      shift_size=-window_size//2,\n",
    "                                                      generate_mask=True)\n",
    "\n",
    "    #print(shifted_w_msa_input.shape) # [bs, num_window, num_patch_in_window, patch_depth]\n",
    "    #print(additive_mask.shape) # [bs*num_window, num_patch_in_window, num_patch_in_window]\n",
    "\n",
    "    shifted_w_msa_input = shifted_w_msa_input.reshape(bs*num_window, num_patch_in_window, patch_depth)\n",
    "   \n",
    "    attn_prob, output = mhsa(shifted_w_msa_input, additive_mask=additive_mask)\n",
    "    \n",
    "    output = output.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    \n",
    "    output, _ = shift_window(output, window_size, shift_size=window_size//2, generate_mask=False)\n",
    "    #print(output.shape) #[bs, num_window, num_patch_in_window, patch_depth]\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de4cd2",
   "metadata": {},
   "source": [
    "## 5、如何构建Patch Merging？\n",
    "- 将window格式的特征转换成图片patch格式\n",
    "- 利用unfold操作，按照merge_size\\*merge_size的大小得到新的patch, 形状为[bs, num_patch_new, merge_size\\*merge_size\\*patch_depth_old]\n",
    "- 使用一个全连接层对depth进行降维成0.5倍，也就是从merge_size\\*merge_size\\*patch_depth_old 映射到 0.5\\*merge_size\\*merge_size\\*patch_depth_old\n",
    "- 输出的是patch embedding的形状格式, [bs, num_patch, patch_depth]\n",
    "- 举例说明：以merge_size=2为例，经过PatchMerging后，patch数目减少为之前的1/4，但是depth增大为原来的2倍，而不是4倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00630b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 难点4 patch merging \n",
    "class PatchMerging(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim, merge_size, output_depth_scale=0.5):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.merge_size = merge_size\n",
    "        self.proj_layer = nn.Linear(\n",
    "            model_dim*merge_size*merge_size,\n",
    "            int(model_dim*merge_size*merge_size*output_depth_scale)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        bs, num_window, num_patch_in_window, patch_depth = input.shape\n",
    "        window_size = int(math.sqrt(num_patch_in_window))\n",
    "\n",
    "        input = window2image(input) #[bs, patch_depth, image_h, image_w]\n",
    "\n",
    "        merged_window = F.unfold(input, kernel_size=(self.merge_size, self.merge_size),\n",
    "                                 stride=(self.merge_size, self.merge_size)\n",
    "                                ).transpose(-1, -2)\n",
    "        merged_window = self.proj_layer(merged_window) #[bs, num_patch, new_patch_depth]\n",
    "\n",
    "        return merged_window\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ce282",
   "metadata": {},
   "source": [
    "## 6、如何构建SwinTransformerBlock？\n",
    "- 每个block包含LayerNorm、W-MHSA、MLP、SW-MHSA、残差连接等模块\n",
    "- 输入是patch embedding格式\n",
    "- 每个MLP包含两层，分别是4\\*model_dim和model_dim的大小\n",
    "- 输出的是window的数据格式，[bs, num_window, num_patch_in_window, patch_depth]\n",
    "- 需要注意残差连接对数据形状的要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe6e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_dim, window_size, num_head):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm4 = nn.LayerNorm(model_dim)\n",
    "\n",
    "        \n",
    "        self.wsma_mlp1 = nn.Linear(model_dim, 4*model_dim)\n",
    "        self.wsma_mlp2 = nn.Linear(4*model_dim, model_dim)\n",
    "        self.swsma_mlp1 = nn.Linear(model_dim, 4*model_dim)\n",
    "        self.swsma_mlp2 = nn.Linear(4*model_dim, model_dim)\n",
    "        \n",
    "        self.mhsa1 = MultiHeadSelfAttention(model_dim, num_head)\n",
    "        self.mhsa2 = MultiHeadSelfAttention(model_dim, num_head)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        bs, num_patch, patch_depth = input.shape\n",
    "\n",
    "        input1 = self.layer_norm1(input)\n",
    "        w_msa_output = window_multi_head_self_attention(input, self.mhsa1, window_size=4, num_head=2)\n",
    "        bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "        w_msa_output = input + w_msa_output.reshape(bs, num_patch, patch_depth)\n",
    "        output1 = self.wsma_mlp2(self.wsma_mlp1(self.layer_norm2(w_msa_output)))\n",
    "        output1 += w_msa_output\n",
    "\n",
    "        input2 = self.layer_norm3(output1)\n",
    "        input2 = input2.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "        sw_msa_output = shift_window_multi_head_self_attention(input2, self.mhsa2, window_size=4, num_head=2)\n",
    "        sw_msa_output = output1 + sw_msa_output.reshape(bs, num_patch, patch_depth)\n",
    "        output2 = self.swsma_mlp2(self.swsma_mlp1(self.layer_norm4(sw_msa_output)))\n",
    "        output2 += sw_msa_output\n",
    "\n",
    "        output2 = output2.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "\n",
    "        return output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135373e",
   "metadata": {},
   "source": [
    "## 7、如何构建SwinTransformerModel？\n",
    "- 输入是图片\n",
    "- 首先对图片进行分块并得到Patch embedding\n",
    "- 经过第一个stage\n",
    "- 进行patch merging，再进行第二个stage\n",
    "- 以此类推...\n",
    "- 对最后一个block的输出转换成patch embedding的格式,[bs, num_patch, patch_depth]\n",
    "- 对patch embedding在时间维度进行平均池化，并映射到分类层得到分类的logits，完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4479c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_image_channel=3, patch_size=4, model_dim_C=8, num_classes=10,\n",
    "                 window_size=4, num_head=2, merge_size=2):\n",
    "        \n",
    "        super(SwinTransformerModel, self).__init__()\n",
    "        patch_depth = patch_size*patch_size*input_image_channel\n",
    "        self.patch_size = patch_size\n",
    "        self.model_dim_C = model_dim_C\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.patch_embedding_weight = nn.Parameter(torch.randn(patch_depth, model_dim_C)) \n",
    "        \n",
    "        self.block1 = SwinTransformerBlock(model_dim_C, window_size, num_head)\n",
    "        self.block2 = SwinTransformerBlock(model_dim_C*2, window_size, num_head)\n",
    "        self.block3 = SwinTransformerBlock(model_dim_C*4, window_size, num_head)\n",
    "        self.block4 = SwinTransformerBlock(model_dim_C*8, window_size, num_head)\n",
    "\n",
    "        \n",
    "        self.patch_merging1 = PatchMerging(model_dim_C, merge_size)\n",
    "        self.patch_merging2 = PatchMerging(model_dim_C*2, merge_size)\n",
    "        self.patch_merging3 = PatchMerging(model_dim_C*4, merge_size)\n",
    "        \n",
    "        self.final_layer = nn.Linear(model_dim_C*8, num_classes)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        patch_embedding_naive = image2emb_naive(image, self.patch_size, self.patch_embedding_weight)\n",
    "        #print(patch_embedding_naive)\n",
    "        \n",
    "        #kernel = self.patch_embedding_weight.transpose(0, 1).reshape((-1, ic, patch_size, patch_size)) # oc*ic*kh*kw\n",
    "        #patch_embedding_conv = image2emb_conv(image, kernel, self.patch_size) # 二维卷积的方法得到embedding\n",
    "        #print(patch_embedding_conv)\n",
    "        \n",
    "        # block1\n",
    "        patch_embedding = patch_embedding_naive\n",
    "        print(patch_embedding.shape)\n",
    "        \n",
    "        sw_msa_output = self.block1(patch_embedding)\n",
    "        print(\"block1_output\", sw_msa_output.shape) #[bs, num_window, num_patch_in_window, patch_depth]\n",
    "        \n",
    "        merged_patch1 = self.patch_merging1(sw_msa_output)\n",
    "        sw_msa_output_1 = self.block2(merged_patch1)\n",
    "        print(\"block2_output\", sw_msa_output_1.shape)\n",
    "        \n",
    "        merged_patch2 = self.patch_merging2(sw_msa_output_1)\n",
    "        sw_msa_output_2 = self.block3(merged_patch2)\n",
    "        print(\"block3_output\", sw_msa_output_2.shape)\n",
    "        \n",
    "        merged_patch3 = self.patch_merging3(sw_msa_output_2)\n",
    "        sw_msa_output_3 = self.block4(merged_patch3)\n",
    "        print(\"block4_output\", sw_msa_output_3.shape)\n",
    "        \n",
    "        bs, num_window, num_patch_in_window, patch_depth = sw_msa_output_3.shape\n",
    "        sw_msa_output_3 = sw_msa_output_3.reshape(bs, -1, patch_depth) #[bs, num_patch, patch_depth]\n",
    "        \n",
    "        pool_output = torch.mean(sw_msa_output_3, dim=1) #[bs, patch_depth]\n",
    "        logits = self.final_layer(pool_output) #[bs, num_classes]\n",
    "        print(\"logits\", logits.shape)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a619fbb",
   "metadata": {},
   "source": [
    "## 8、模型测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eae29916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4096, 8])\n",
      "block1_output torch.Size([4, 256, 16, 8])\n",
      "block2_output torch.Size([4, 64, 16, 16])\n",
      "block3_output torch.Size([4, 16, 16, 32])\n",
      "block4_output torch.Size([4, 4, 16, 64])\n",
      "logits torch.Size([4, 10])\n",
      "tensor([[-1.3811e-02, -9.8207e-02, -1.2010e-01,  8.1500e-02, -9.5337e-02,\n",
      "          2.3576e-01,  5.3743e-01,  2.7579e-02, -3.6139e-01, -5.6262e-02],\n",
      "        [ 7.3799e-02,  5.1983e-05, -2.8325e-01,  2.9645e-01, -2.2172e-01,\n",
      "          1.9904e-01,  4.3626e-01,  1.5317e-01, -2.7202e-02, -2.2374e-01],\n",
      "        [ 7.9339e-02,  2.4252e-02, -2.0669e-01, -1.8361e-03, -1.9762e-01,\n",
      "          2.1630e-01,  6.1755e-02,  1.7363e-01, -2.1660e-01, -2.5461e-01],\n",
      "        [ 3.5126e-01, -1.8232e-03, -3.7366e-01,  2.4457e-01, -1.4654e-03,\n",
      "          4.9239e-01,  8.3178e-02, -8.6517e-02, -1.6285e-02, -2.3550e-01]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 难点5 分类模块，输出类别为num_classes的logits\n",
    "if __name__ == \"__main__\":\n",
    "    bs, ic, image_h, image_w = 4, 3, 256, 256\n",
    "    patch_size = 4\n",
    "    model_dim_C = 8 #一开始的patch embedding的大小\n",
    "    # max_num_token = 16\n",
    "    num_classes = 10\n",
    "    window_size = 4\n",
    "    num_head = 2\n",
    "    merge_size = 2\n",
    "    \n",
    "    patch_depth = patch_size*patch_size*ic\n",
    "    \n",
    "    image = torch.randn(bs, ic, image_h, image_w)\n",
    "    \n",
    "    model = SwinTransformerModel(ic, patch_size, model_dim_C, num_classes,\n",
    "                                 window_size, num_head, merge_size)\n",
    "    logits = model(image)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39dec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
